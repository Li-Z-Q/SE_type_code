['/home/lizhuoqun/SE_type_code', '/home/lizhuoqun/anconda/envs/py38/lib/python38.zip', '/home/lizhuoqun/anconda/envs/py38/lib/python3.8', '/home/lizhuoqun/anconda/envs/py38/lib/python3.8/lib-dynload', '/home/lizhuoqun/anconda/envs/py38/lib/python3.8/site-packages', '/home/lizhuoqun/SE_type_code/data', '/home/lizhuoqun/SE_type_code/models', '/home/lizhuoqun/SE_type_code/tools', '/home/lizhuoqun/SE_type_code/resource', '/home/lizhuoqun/SE_type_code/pre_train']
sentence level BiLSTM extra
___________****************************************** 300 ********************************************************************************
paragraph level BiLSTM label embedding MLP pre
Namespace(BATCH_SIZE=128, DROPOUT=0.5, EPOCHs=40, LEARN_RATE=0.001, WEIGHT_DECAY=0.0001, cheat='False', fold_num=0, mask_p=1.0)




time= 0
self.BiLSTM_1_require_grad:  True
self.cheat:  False
self.mask_p:  1.0
self.threshold:  0.8
start to get stanford
already get stanford
start get bert_tokenizer
get bert tokenizer
if_do_embedding:  True
start read data catalogue
start get train data
already deal 10 file
already deal 20 file
already deal 30 file
already deal 40 file
already deal 50 file
already deal 60 file
already deal 70 file
already deal 80 file
already deal 90 file
already deal 100 file
already deal 110 file
already deal 120 file
already deal 130 file
already deal 140 file
already deal 150 file
already deal 160 file
already deal 170 file
already deal 180 file
already deal 190 file
already deal 200 file
already deal 210 file
already deal 220 file
already deal 230 file
already deal 240 file
already deal 250 file
already deal 260 file
already deal 270 file
already deal 280 file
len(train_valid_data_list):  3368
already deal 10 file
already deal 20 file
already deal 30 file
already deal 40 file
already deal 50 file
already deal 60 file
already deal 70 file
already deal 80 file
complete get data, len(test_data_list):  919


start sentence level BiLSTM  extra


epoch 0/4


epoch 1/4


epoch 2/4


epoch 3/4
sentence level BiLSTM  extra best_epoch:  3 -1 -1
sentence level BiLSTM  extra end

********************************************


epoch 0/40
model.reset
self.reset_num:  0
self.cheat:  False
self.mask_p:  1.0
self.valid_flag:  False
self.BiLSTM_1_require_grad:  True
Traceback (most recent call last):
  File "run_paragraph_level_BiLSTM_label_embedding_MLP_pre.py", line 72, in <module>
    best_epoch, best_model, best_macro_Fscore, best_acc = train_and_valid(model, optimizer, train_batch_list,
  File "/home/lizhuoqun/SE_type_code/train_valid_test/train_valid_paragraph_level_model.py", line 33, in train_and_valid
    pre_labels_list, loss = model.forward(sentences_list, gold_labels_list)  # sentence_num * 7
  File "/home/lizhuoqun/SE_type_code/models/paragraph_level_BiLSTM_label_embedding_MLP_pre.py", line 101, in forward
    ex_pre_label, _, sentence_embedding, softmax_output = self.BiLSTM_1(word_embeddings_list, 0)  # 1 * 300
  File "/home/lizhuoqun/anconda/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/lizhuoqun/SE_type_code/models/sentence_level_BiLSTM_extra.py", line 28, in forward
    BiLSTM_output, _ = self.BiLSTM(word_embeddings_list, init_hidden)  # 1 * sentence_len * 300
  File "/home/lizhuoqun/anconda/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/lizhuoqun/anconda/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py", line 679, in forward
    result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED
